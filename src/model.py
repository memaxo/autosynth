"""
MLX model integration for AutoSynth using Mistral Small
"""

from typing import Any, List, Optional, Dict
from langchain_community.llms.mlx_pipeline import MLXPipeline
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.pydantic_v1 import Field, root_validator

class MistralValidator(LLM):
    """
    Custom LLM class for Mistral Small using MLX
    """
    
    model_path: str = "mlx-community/Mistral-Small-24B-Instruct-2501-4bit"
    max_tokens: int = Field(default=100)
    temperature: float = Field(default=0.1)
    pipeline: Optional[MLXPipeline] = None
    
    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that MLX pipeline is properly initialized"""
        try:
            values["pipeline"] = MLXPipeline.from_model_id(
                values["model_path"],
                pipeline_kwargs={
                    "max_tokens": values["max_tokens"],
                    "temp": values["temperature"]
                }
            )
        except Exception as e:
            raise ValueError(
                f"Failed to initialize MLX pipeline with error: {str(e)}"
            )
        return values
        
    @property
    def _llm_type(self) -> str:
        """Return type of LLM."""
        return "mistral_mlx"
        
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        Run MLX inference on the input prompt.
        
        Args:
            prompt: The prompt to pass into the model
            stop: A list of strings to stop generation when encountered
            run_manager: Optional callback manager
            
        Returns:
            The string generated by the model
        """
        if stop is not None:
            raise ValueError("Stop sequences are not supported by MLX pipeline")
            
        # Format prompt for chat model
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = self.pipeline.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True
        )
        
        # Generate response
        response = self.pipeline(formatted_prompt)
        
        return response.strip()
        
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "model_path": self.model_path,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        } 